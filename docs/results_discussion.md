# Results & Discussion (Template)

## Setup Summary
- Runs: note dates, run_ids, subreddits, keywords, modes (mock/live), and posting enabled? (y/n).
- Volume: number of posts scanned, matches surfaced, approvals given, comments posted.

## Engagement Findings
- Per-comment stats: score, replies_count (from `bot_metrics.csv`).
- Observations: which topics/keywords saw engagement; any patterns in upvotes/replies.
- Note limitations (small N, manual approval bias, subreddit norms).

## Content Quality & Safety
- Any human rejections? Why (off-topic, safety concerns, redundancy)?
- Instances where stub/LLM reply felt inadequate; proposed prompt tweaks.

## Discussion
- How the human-in-the-loop affected safety and relevance.
- Practical constraints (API access, rate limits, moderation considerations).
- Risks and mitigations observed in practice.

## Next Steps
- Prompt/keyword refinements.
- Additional metrics (sentiment, longer-tail follow-up).
- Policy/ethics adjustments based on observed edge cases.
